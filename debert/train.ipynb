{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1. Load dataset ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/content/masterList_relabelled_v2.csv\")\n",
    "df = df.dropna(subset=[\"text\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "\n",
    "# --- Step 2. Train/validation split ---\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# --- Step 3. Tokenizer ---\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset  = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# --- Step 4. Compute dynamic class weights ---\n",
    "labels = df[\"label\"].values\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "print(\"Class Weights:\", dict(zip(np.unique(labels), weights)))\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(\"cuda\")\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# --- Step 5. Load Model ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4).to(\"cuda\")\n",
    "\n",
    "# --- Step 6. Metrics ---\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    macro_f1 = precision_recall_fscore_support(labels, preds, average=\"macro\")[2]\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1_weighted\": f1, \"f1_macro\": macro_f1,\n",
    "            \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# --- Step 7. Custom Trainer ---\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\").to(torch.long)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- Step 8. Training arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_deberta\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",      \n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_deberta\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# --- Step 9. Trainer ---\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- Step 10. Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Step 11. Evaluate ---\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=[\"Valid (0)\", \"Spam (1)\", \"LowQuality (2)\", \"Rant (3)\"]\n",
    "))\n",
    "\n",
    "# --- Step 12. Confusion Matrix ---\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Valid (0)\", \"Spam (1)\", \"LowQuality (2)\", \"Rant (3)\"],\n",
    "            yticklabels=[\"Valid (0)\", \"Spam (1)\", \"LowQuality (2)\", \"Rant (3)\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - DeBERTa v3 Base\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
